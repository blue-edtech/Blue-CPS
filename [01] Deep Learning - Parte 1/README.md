# Aula #1 - Deep Learning - Parte 1

Por <a href="https://www.youtube.com/paulosalvatore" target="_blank">Paulo¬†Salvatore</a> - Head de Produtos <a href="https://blueedtech.com.br/quem-somos/" target="_blank">@Blue¬†Edtech</a> & Chanely Marques - Eterna Aprendiz <a href="https://blueedtech.com.br/quem-somos/" target="_blank">@Blue Edtech</a>

Ol√° :wave:, seja bem-vindo(a) ao primeiro v√≠deo desta s√©rie. Nosso objetivo √© introduzir o conceito de Rede Neural, neur√¥nios, aprendizagem profunda, fun√ß√µes de ativa√ß√£o, algoritmo de _Backpropagation_ e aplica√ß√£o pr√°tica com _TensorFlow Playground_.

## Introdu√ß√£o

Antes de mergulharmos em **_Deep Learning_** (D.L. - aprendizagem profunda) √© necess√°rio sabermos que ele est√° dentro de uma estrutura de conhecimentos da **Intelig√™ncia Artificial** (I.A.).

![Aula01_Figura01](imagens/Aula01_Figura01.png)

A I.A. come√ßou sua evolu√ß√£o pela d√©cada de 50 e faz parte do campo de estudos da Ci√™ncia da Computa√ß√£o que se combina a conjuntos de dados robustos para permitir a resolu√ß√£o de problemas. Alan Turing, muitas vezes chamado de "pai da ci√™ncia da computa√ß√£o", frequentemente questionava se as m√°quinas seriam capazes de pensar. A partir da√≠ definiu que a I.A. seria categorizada como <a href="https://www.ibm.com/br-pt/cloud/learn/what-is-artificial-intelligence" target="_blank">"sistemas que agem como humanos"</a>.

O **_Machine Learning_** (M.L. - aprendizagem de m√°quina) veio um pouquinho depois, se tornando uma t√©cnica de aprendizagem de m√°quina para que ela reconhe√ßa padr√µes e aprenda em cima de um grande conjunto de dados e a partir deste aprendizado tomamos decis√µes, fazemos previs√µes, entre outros.

O _Deep Learning_ fez sua primeira apari√ß√£o entre as d√©cadas de 70 e 80 e come√ßou a se popularizar a partir de 2010. √â um m√©todo que incorpora redes neurais em camadas para que a aprendizagem a partir dos dados seja feita de forma iterativa.

![Aula01_Figura02](imagens/Aula01_Figura02.png)

Tanto M.L. quanto o D.L recebem informa√ß√µes na camada de **Input** (entrada) e, baseados nestas informa√ß√µes, tomam decis√µes ou fazem alguma previs√£o na camada de **Output** (sa√≠da), comparando aos dados que j√° eram conhecidos.

No processo de M.L., os dados entram pela camada de **entrada**, e a extra√ß√£o deles √© feita completamente por um Ser Humano. Estruturados e organizados, estes dados s√£o classificados por uma m√°quina, gerando um resultado na camada de **sa√≠da**.

> ***_NOTA :clipboard: :pencil2: :_***  O processo de extra√ß√£o se caracteriza por observar, analisar e estruturar o conjunto de dados recebidos na camada **de entrada**.

> ***_NOTA :clipboard: :pencil2: :_***  O processo de classifica√ß√£o √© feito inteiramente por uma m√°quina, atrav√©s de uma ou um conjunto de f√≥rmulas matem√°ticas classificando todas as _features_ extra√≠das.

No processo de D.L., os dados entram pela camada de **entrada**, os processos de extra√ß√£o e classifica√ß√£o das _features s√£o_ feitos inteiramente pela m√°quina gerando a **sa√≠da**. A grande vantagem √© que ele pode nos trazer informa√ß√µes interessantes e/ou relevantes, que poderiam passar despercebidas pelo Ser Humano.

> ***_Importante :mega: :_*** Para ambos os processos, os dados precisam ser preparados por um Ser Humano para entrar no processo de aprendizagem.

Para que estes conceitos fiquem mais claros, assista aos v√≠deos a seguir:

- Este carro aut√¥nomo usa o D.L. para saber se objetos que passam no campo de vis√£o s√£o carros ou n√£o: <a href="https://www.youtube.com/watch?v=mUV5ZwIC9_g&feature=youtu.be" target="_blank">Carro¬†Aut√¥nomo</a>.

- No reconhecimento de voz as palavras s√£o identificadas atrav√©s da inser√ß√£o de v√°rios arquivos de voz: <a href="https://www.youtube.com/watch?v=NaqZkV_fBIM&feature=youtu.be" target="_blank">Reconhecimento¬†de¬†Voz</a>.

- Projeto GauGAN da Nvidia que transforma desenhos em paisagens usando Redes Neurais: <a href="https://www.youtube.com/watch?v=p5U4NgVGAwg" target="_blank">GauGAN</a>.

- Poder de an√°lise de dados entre CPU vs. GPU: <a href="https://www.youtube.com/watch?v=-P28LKWTzrI" target="_blank">CPU¬†vs. GPU</a>.

### Formas de An√°lise de Inser√ß√£o de Dados

Neste exemplo, somos um algoritmo de I.A. com o objetivo de entender informa√ß√µes:

![Aula01_Figura03](imagens/Aula01_Figura03.png)

A letra `A` pode ser entendida de diversas formas:

- Ao analisar sua imagem, independente da forma que foi escrita
  - No caso do computador, uma imagem √© um conjunto de pixels
  - No caso do ser humano, uma imagem √© gerada pelos nossos olhos e processada pelo c√©rebro
- Ouvindo-a sendo dita
  - No caso do computador, um √°udio √© um arquivo que cont√©m v√°rias faixas de frequ√™ncias (medidas em Hertz) e intensidades diferentes para cada valor de frequ√™ncia
  - No caso do ser humano, um som √© interpretado por nossos ouvidos e processado pelo c√©rebro

Para que, ao final do processo, tenhamos a informa√ß√£o que aquela letra representa, precisamos transform√°-la, pois, afinal, n√£o √© t√£o pr√°tico usar um conjunto de pixels ou um arquivo de √°udio em um programa de computador, trabalhar diretamente com a informa√ß√£o √© muito melhor e mais pr√°tico.

O mesmo processo √© feito para d√≠gitos num√©ricos:

![Aula01_Figura04](imagens/Aula01_Figura04.png)

O d√≠gito pode ter sido escrito √† m√£o, digitado em um computador ou extra√≠do de uma imagem. Como algoritmos, precisamos ler estes **dados** para saber que o d√≠gito √© um `5` e n√£o uma representa√ß√£o dele.

Este mesmo processo de `Entrada de dados -> An√°lise -> Sa√≠da` √© feito pelo nosso c√©rebro o tempo todo. Atrav√©s da Rede Neural, espalhada por todo o corpo √© poss√≠vel captarmos diversas informa√ß√µes do ambiente, as transformamos em est√≠mulos sensoriais que s√£o entendidos e traduzidos pelos neur√¥nios no c√©rebro, que nos devolve a informa√ß√£o em forma de sensa√ß√£o de tato, audi√ß√£o, vis√£o ou paladar e a partir disso tomamos alguma decis√£o.

## Prepara√ß√£o da Rede Neural

<!-- 13:08 -->

Para que o c√©rebro consiga fazer todas as coisas que hoje sabemos que √© capaz de fazer, podemos imaginar que um √∫nico neur√¥nio t√™m sua estrutura e forma de funcionar bastante complexa. 

Por hora n√£o vamos nos servir desta complexidade, mas sim de sua simplicidade funcional: receber e transmitir impulsos el√©tricos, ou seja, receber e transmitir informa√ß√µes.

![Aula01_Figura06](imagens/Aula01_Figura06.png)

A estrutura b√°sica de um neur√¥nio consiste em:

- **Dendritos**: √© por aqui que as informa√ß√µes entram - `Input`;
- **Corpo celular**: recebidas as informa√ß√µes, as mesmas ser√£o analisadas, processadas e decididas quanto ao destino;
- **Ax√¥nio envolto de ilhas de bainha de mielina**: fio condutor respons√°vel por transmitir as informa√ß√µes do corpo celular para o ax√¥nio terminal, e;
- **Ax√¥nio terminal**:  √© por aqui que as informa√ß√µes saem - `Output`.

Para que as informa√ß√µes saiam do ax√¥nio terminal `A` para o dendrito do ax√¥nio `B`, uma estrutura chamada **sinapse** ‚Äî que pode ser qu√≠mica ou el√©trica ‚Äî √© ativada atrav√©s do est√≠mulo que sai do ax√¥nio terminal de `A` para o dendrito de `B`.

Gra√ßas √† essas sinapses, nosso c√©rebro √© capaz de processar 11 bilh√µes de bits por segundo atrav√©s de conex√µes paralelas entre todos os neur√¥nios.

> ***_Curiosidade :brain: :_***  A estrutura sinapse se forma entre um neur√¥nio do c√©rebro com outro neur√¥nio do c√©rebro. Quando um neur√¥nio do c√©rebro precisa falar com uma c√©lula muscular para ativar qualquer m√∫sculo no corpo, a estrutura se chama **jun√ß√£o neuromuscular**.

Este conceito de processar informa√ß√µes paralelamente √© bastante aplicado √† computa√ß√£o. Chamado de **processamento paralelo**, ele explora e usa simultaneamente v√°rias unidades de processamento (CPU's) para aumentar a velocidade.

Por exemplo, ao ouvirmos a letra `A`, a informa√ß√£o do som (em Hz) √© transmitida pelos condutos auditivos e captada pelos neur√¥nios especializados. Estes captadores formam uma cascata de est√≠mulos el√©tricos ativando o processamento paralelo nas √°reas cerebrais especializadas em audi√ß√£o, para que finalmente o reconhecimento da informa√ß√£o `A` seja feito e devolvido para n√≥s em forma de "Hmm, isso que eu ouvi √© a letra `A`, eu a conhe√ßo!".

Este mesmo processo √© feito para todas as informa√ß√µes que captamos do ambiente atrav√©s dos 5 sentidos.

> Por hora, queremos que voc√™ guarde o seguinte: podemos fazer uma analogia de tudo que vimos acima a um √∫nico neur√¥nio de m√°quina. Na m√°quina, este neur√¥nio realiza uma fun√ß√£o muito simples que √© a de armazenar um n√∫mero que, a princ√≠pio, ser√° entre 0 e 1.

![Aula01_Figura05](imagens/Aula01_Figura05.png)

## Representa√ß√£o Matem√°tica

<!-- 16:59 -->

Vamos observar e estudar com calma a figura abaixo:

![Aula01_Figura07](imagens/Aula01_Figura07.png)

Paralelamente √† estrutura funcional de um neur√¥nio, esta representa√ß√£o matem√°tica possui uma regi√£o que recebe **valores de entrada** (equivalente aos dendritos), outra que **analisa, processa e envia as informa√ß√µes** (equivalente ao corpo celular e ax√¥nio) e, por fim, a que faz a **sa√≠da** (equivalente ao ax√¥nio terminal).

Vamos trabalhar novamente o exemplo da letra `A`:

- a informa√ß√£o entrar√° na camada Roxa, de entrada;
- ser√° processada na camada Laranja, e;
- sair√° pela camada Verde, de sa√≠da, nos trazendo o resultado se a letra `A` foi ou n√£o reconhecida.

> ***_Importante :mega: :_*** Neste momento precisamos ter muito claro que a camada de entrada √© por onde entram as informa√ß√µes e a camada de sa√≠da √© por onde saem.

### Processamento das informa√ß√µes

Dentre todos os conceitos de uma Rede Neural, certamente o entendimento sobre **pesos (_weights_)** e **valores propagados (_bias_)** s√£o fundamentais (talvez a parte mais importante da uma Rede Neural üôÇ).

Quando a informa√ß√£o √© transmitida da camada de entrada para a de processamento, os pesos s√£o aplicados √† esta informa√ß√£o, somados e passados adiante para uma fun√ß√£o de ativa√ß√£o, juntamente com os valores de Bias.

:key: :bulb: Cada **informa√ß√£o que entra** pela camada de entrada √© **multiplicada** por um **peso** (que inicialmente corresponde a um valor aleat√≥rio) antes de avan√ßar pela Rede Neural.

![Aula01_Figura08](imagens/Aula01_Figura08.png)

:key: :bulb: Ap√≥s essa multiplica√ß√£o da **informa√ß√£o que entra** vezes o **peso**, aplicamos em um **somat√≥rio**, que representa a soma de todos os valores.

Por exemplo, vamos imaginar que recebemos na camada de entrada as letras `A` e `B`, cada uma com o seguinte valor:

- `Letra A` = `2`
- `Letra B` = `5`

Para cada letra, recebemos tamb√©m um **peso**, com os seguintes valores:

- `Peso A` = `3`
- `Peso B` = `6`

Com isso, temos os seguinte c√°lculo para saber o valor do **somat√≥rio**:

- `(Letra A * Peso A) + (Letra B * Peso B)`

Substituindo pelos valores:

- `(2 * 3) + (5 * 6)` que √© igual a `36`

Portanto, o valor do **somat√≥rio** √© `36`.

Agora, podemos somar um valor de **_bias_** ‚Äî que pode ser um n√∫mero positivo ou negativo ‚Äî e nesse caso ser√° `-2`.

- `Somat√≥rio + Bias`
- `36 + -2` √© igual a `34`

Portanto, o resultado corresponde a `34`.

Obtido o resultado final, este ser√° passado por uma fun√ß√£o de ativa√ß√£o e, a partir dela, teremos um novo n√∫mero que ser√° passado para a camada de sa√≠da.

Vamos entrar mais a fundo em cada uma dessas partes?

<!-- 21:55 -->

## _Dataset_ MNIST

Para aprofundarmos nosso estudo e deix√°-lo mais tang√≠vel, vamos falar sobre um banco de dados bastante espec√≠fico chamado **MNIST** que cont√©m imagens bin√°rias de d√≠gitos escritos √† m√£o por diversas pessoas.

![Aula01_Figura09](imagens/Aula01_Figura09.png)

> ***_Importante :mega: :_*** Desenvolver uma Rede Neural capaz de compreender os d√≠gitos destas imagens √© considerado o primeiro exemplo a ser aprendido por quem quer entender este universo, equivalente ao `Hello World!` no universo da programa√ß√£o. (PS: atente-se √† maldi√ß√£o :jack_o_lantern:)

O MNIST se tornou uma estrutura bem difundida entre a comunidade _Deep Learning_. Diversos outros projetos seguem a mesma base de imagens em escala de cinza, de tamanho 28x28 pixels, com 60.000 arquivos para treino e 10.000 arquivos para teste, organizadas em 10 classes distintas.

No caso do MNIST, cada classe distinta representa um dos n√∫meros de `0 a 9`.

### Fashion MNIST

O **[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)** √© um banco de dados de imagens que segue a estrutura do MNIST, possui imagens de pe√ßas de roupa, tamb√©m organizadas em 10 classes distintas.

![Aula01_Figura10](imagens/Aula01_Figura10.png)

No v√≠deo-exemplo abaixo, podemos analisar como um algoritmo observa um conjunto de dados com artigos de moda em geral e os posiciona por similaridade.

![Aula1_Gif01](imagens/Aula01_Gif01.gif)

## E como as imagens s√£o interpretadas?

<!-- 23:57 -->

Das 70.000 imagens que podemos encontrar no MNIST, vamos pegar uma delas para o nosso exerc√≠cio. Essa imagem possui um tamanho de `28px` de largura por `28px` de altura, totalizando `784px`, organizados em linhas e colunas, da seguinte forma:

![Aula01_Figura11](imagens/Aula01_Figura11.png)

Cada pixel na imagem √© an√°logo a um neur√¥nio, contendo n√∫meros que v√£o do intervalo de 0 a 1.

![Aula01_Figura12](imagens/Aula01_Figura12.png)

Quanto mais pr√≥ximo o neur√¥nio estiver do n√∫mero `1`, significa que ele est√° sendo mais ativado. Observe na imagem que a cor fica mais branca com o n√∫mero `1.0` dentro dele, enquanto os outros pontos est√£o mais claros (entre `0.01` e `0.99`) ou at√© mesmo completamente pretos (`0.0`).

> ***_NOTA :clipboard: :pencil2: :_***  No _dataset_ do MNIST, todas as imagens acompanham o significado do que ela representa. No nosso exemplo, a imagem recebe o significado `3`.

## E quando os arquivos n√£o s√£o imagens?

Para uma an√°lise em que a camada de entrada receba um arquivo de √°udio, a an√°lise ser√° realizada da mesma forma, observando-se a frequ√™ncia e sua intensidade.

![Aula01_Figura13](imagens/Aula01_Figura13.png)

Cada frequ√™ncia carrega consigo uma intensidade, gerando uma sa√≠da que pode ser uma letra qualquer ou uma frase, por exemplo.

## Modelo de Rede Neural

<!-- 27:47 -->

Semelhante √† representa√ß√£o do neur√¥nio, o modelo de Rede Neural carrega a mesma estrutura.

![Aula01_Figura14](imagens/Aula01_Figura14.png)

- **Camada de entrada (_Input_):** √© a primeira camada da Rede Neural, respons√°vel por receber as informa√ß√µes que entrar√£o na Rede.

  No nosso exemplo temos os valores `1`, `0.5`e `0.2` entrando no modelo.

- **Camada Oculta (_Hidden_):** nesta camada encontramos os neur√¥nios organizados em n√≥s, interconectados com as camadas de entrada e sa√≠da. Aqui s√£o aplicados os pesos e _bias_ √†s informa√ß√µes recebidas. De forma did√°tica e para nosso entendimento, podemos ver que estes n√≥s est√£o verticalmente empilhados e correspondem a `4` bolinhas azuis.

  > ***_NOTA :clipboard: :pencil2: :_***  √© nesta camada em que a Rede realmente identifica o que est√° sendo recebido.

  O primeiro neur√¥nio da camada oculta recebeu os valores da camada de entrada, aplicou os pesos e o _bias_, alterando-os para o resultado `0.42`, entregando esse resultado para a camada de sa√≠da.

- **Camada de sa√≠da (_Output_):** sendo a √∫ltima camada na rede, recebe o total da camada oculta.

  No nosso exemplo, o valor de sa√≠da modificado e identificado √© de `0.74`.

### Olha s√≥ que interessante! :boom:

Um modelo de Rede Neural que n√£o necessariamente est√° dentro do D.L., mas sim no M.L.

Quando vemos um modelo de Rede Neural com apenas **uma √∫nica camada oculta** estamos olhando para um modelo tradicional de M.L., que usamos em diversos exemplos.

A m√°gica fica maior, complexa e interessante quando adicionamos **mais camadas ocultas** a este modelo, caracterizando uma **Rede Neural Profunda**.

![Aula01_Figura15](imagens/Aula01_Figura15.png)

Logo, a quantidade de camadas ocultas √© o que define se meu modelo se trata de uma Rede Neural Profunda ou apenas uma Rede Neural Normal.

![Aula01_Figura16](imagens/Aula01_Figura16.png)

## Conectando tudo

<!-- 30:31 -->

Voc√™ se lembra que falamos sobre o MNIST alguns passos atr√°s? 

![Aula01_Figura09](imagens/Aula01_Figura09.png)

O banco de dados do MNIST possui um conjunto de treinamento de 60.000 imagens e um conjunto de testes com 10.000, ou seja, um sub-conjunto de um conjunto maior de treinamento dispon√≠vel. 

N√≥s utilizamos o sub-conjunto de 10.000 imagens-teste para comparar o resultado que vamos obtendo durante o treinamento e refinando o algoritmo.

Primeiro, apresentamos as imagens de **treino** ao modelo para que ele aprenda a reconhecer os padr√µes. Com isso, medimos os resultados.

Para garantir que os resultados s√£o coerentes, apresentamos as imagens de **teste** e verificamos se os resultados s√£o t√£o satisfat√≥rios quanto.

> Podemos fazer uma analogia ao treino de caligrafia (pessoa idosa aqui :older_woman:) que faz√≠amos na escola. At√© alcan√ßarmos algo parecido com a letra da professora (√© a meta que a gente almeja, n√©?! kkkrying) treinamos diversas vezes e, ao longo do processo, observamos em nosso caderno centenas de formas diferentes de fazer a mesma letra (todas um horror at√© a perfei√ß√£o, a gente sabe! :sweat:).

Na atividade **treina-compara-testa**, nossa Rede √© capaz de generalizar, abstrair e observar novas imagens, garantindo que os resultados se tornem cada vez mais positivos (algo que pode ser medido na curva de aprendizado de uma Rede Neural).

Na pr√°tica, a imagem que a Rede recebe do MNIST est√° em em preto e branco, normalizada por tamanho e centralizada para caber em uma caixa delimitadora de pixels de `28x28`, totalizando `784px`, e suaviza√ß√£o de borda criando os tons de cinza que vimos anteriormente.

Dentro do _dataset_, organizamos os arquivos das imagens (`Images`) em um local e a representa√ß√£o das imagens (`Labels`) em outro.

![Aula01_Figura17](imagens/Aula01_Figura17.png)

Ap√≥s escolhermos a imagem de treino para o nosso modelo, precisamos realizar um procedimento antes de a colocarmos na camada de entrada, este processo se chama **achatamento**.

Este processo consiste em transformar toda a matriz de `28x28px` em um vetor linear unidimensional, ou seja, em uma √∫nica linha, para conect√°-la com a pr√≥xima camada.

![Aula01_Figura18](imagens/Aula01_Figura18.png)

O vetor √© constru√≠do de forma sequencial, respeitando a ordem das linhas na matriz, ou seja, a primeira linha com 28 colunas √© achatada e colocada na primeira posi√ß√£o do vetor, a segunda tem suas 28 colunas achatas e colocadas na segunda posi√ß√£o do vetor, e assim por diante, gerando uma √∫nica linha com 784 pixels.

Cada um destes pixels √© o que de fato a Rede vai usar para definir a representa√ß√£o da imagem.

Cada pixel da imagem contendo um valor que vai `0` a `1` equivale a um neur√¥nio na camada de entrada.

![Aula01_Figura19](imagens/Aula01_Figura19.png)

Podemos ver que o primeiro pixel vai para o primeiro neur√¥nio da camada e o √∫ltimo para seu neur√¥nio respectivo. Desta forma, temos todos os pixels da matriz representados dentro do modelo.

No nosso modelo de exemplo temos 2 camadas ocultas com 16 neur√¥nios cada.

> ***_NOTA :clipboard: :pencil2: :_***  A quantidade de camadas ocultas e de neur√¥nios s√£o definidas por n√≥s e existem v√°rias t√©cnicas para saber quando adicionar/remover camadas e neur√¥nios.

![Aula01_Figura20](imagens/Aula01_Figura20.png)

## Camada de Sa√≠da (_Output_)

Tudo que foi processado e se tornou relevante pelo nosso modelo deve ser apresentado nesta camada.

![Aula01_Figura21](imagens/Aula01_Figura21.png)

Em um outro exemplo, a nossa imagem de entrada √© a representa√ß√£o do n√∫mero `7`. Ela foi achatada e processada pelo modelo enchendo o neur√¥nio do n√∫mero `7` na camada de sa√≠da.

Se observarmos a figura acima atentamente perceberemos que temos um neur√¥nio para cada n√∫mero escrito √† m√£o contido dentro do _Dataset_.

> ***_Importante :mega: :_*** Identificar o d√≠gito de qualquer uma destas imagens pelo nosso _Deep Learning_ √© uma tarefa an√°loga a um '_Hello World!_'. Atente-se √† maldi√ß√£o. Cada possibilidade na camada de sa√≠da deve ser igualmente proporcional √† quantidade de itens que ser√£o inseridos atrav√©s de representa√ß√µes em nosso modelo.

Quando juntamos todas as camadas do nosso modelo, percebemos que a entrada e sa√≠da est√£o sempre conectadas e relacionadas pelo conjunto de dados.



![Aula01_Figura22](imagens/Aula01_Figura22.png)

Para sabermos se nossa Rede acertou o n√∫mero que inserimos, precisamos identificar o n√∫mero que ela assumiu para cada uma das informa√ß√µes e, a partir disso, vemos se acertou mais ou menos.

Os dados na camada de sa√≠da s√£o fundamentais para sabermos se a Rede est√° acertando ou errando durante o treinamento. Lembrando que, quanto mais o valor do neur√¥nio estiver pr√≥ximo de `1`, mais certo est√° o resultado.

![Aula01_Figura23](imagens/Aula01_Figura23.png)

## Par√¢metros Utilizados - Pesos e _Bias_

<!-- 39:24 -->

Cada um dos 16 neur√¥nios da camada escondida est√° conectado a um pixel recebido na camada de entrada, ou seja, cada neur√¥nio recebe os 784 pixels.

![Aula01_Figura24](imagens/Aula01_Figura24.png)

Cada **peso** multiplicado ao **valor de entrada + _bias_** representa a for√ßa da conex√£o entre os neur√¥nios. Se o peso do neur√¥nio `1` ao `3` for maior que o peso do neur√¥nio `4` ao `7`, ter√° maior influ√™ncia sobre estes.

Por l√≥gica, percebemos que os par√¢metros podem diminuir a **import√¢ncia** dos pixels recebidos na camada de entrada e que ser√£o entregues na camada de sa√≠da, dependendo de sua configura√ß√£o na camada oculta.

![Aula01_Figura25](imagens/Aula01_Figura25.png)

Vamos agora a um exemplo pr√°tico e simples ‚Äî e que vai quebrar a tradi√ß√£o do '_Hello World_' (certamente seremos perdoados pelos fins did√°ticos!).

Neste exemplo, temos um √∫nico pixel com **duas** possibilidade de cores.

A **primeira** possibilidade representa o pixel de cor **preta**.

A **segunda** possibilidade representa o pixel de cor **branca**.

Note que a **camada de entrada** tem um √∫nico ponto e a de **sa√≠da** dois pontos, correspondentes ao preto e ao branco.

![Aula01_Figura26](imagens/Aula01_Figura26.png)

Quanto mais pr√≥ximo de `1`, mais o neur√¥nio de cor preta foi ativado na camada de sa√≠da.

Quanto mais pr√≥ximo de `1`, mais o neur√¥nio de cor branca foi ativado na camada de sa√≠da.

Sim, existe a possibilidade dos neur√¥nios serem ativados juntos üòÆ!

Recebemos ent√£o, uma imagem com um pixel preto. Chamaremos esse √∫nico pixel de **um par√¢metro**.

Esse **par√¢metro de entrada** est√° conectado aos neur√¥nios na camada oculta. A esta conex√£o, damos o nome de **peso**.

> ***_Para lembrar :thought_balloon: :_***Para cada um dos neur√¥nios, multiplicamos par√¢metro e peso.

> ***_Para lembrar :thought_balloon: :_***Os pesos podem ser positivos ou negativos, dando mais ou menos for√ßa √†quela conex√£o.

A camada oculta vai tentar entender a informa√ß√£o recebida em diversos n√≠veis, dependendo do que foi configurado.

Cada um dos neur√¥nios da camada oculta tamb√©m se conectam aos da camada de sa√≠da. Em cada conex√£o, tamb√©m temos um novo valor de pesos, que definir√° as ativa√ß√µes finais. Essas ativa√ß√µes finais representam o que de fato a Rede identificou.

## Combina√ß√£o de Imagens para Formar um D√≠gito

<!-- 46:10 -->

Voltando ao exemplo do MNIST, vamos analisar as imagens abaixo:

![Aula01_Figura27](imagens/Aula01_Figura27.png)

Perceba que a imagem do n√∫mero `9` pode ser dividida em dois fragmentos, no primeiro temos um c√≠rculo, e no segundo um tra√ßo na vertical.

Um neur√¥nio √© respons√°vel por identificar o `primeiro fragmento` e outro neur√¥nio o `segundo fragmento`.

Juntos, ativam o neur√¥nio do `n√∫mero 9` na camada de sa√≠da. 

![Aula01_Figura28](imagens/Aula01_Figura28.png)

Para a imagem do n√∫mero `8` temos novamente dois fragmentos ‚Äî o mesmo c√≠rculo do `9` ‚Äî na por√ß√£o superior, e um outro c√≠rculo menor na por√ß√£o inferior.

> ***_NOTA :clipboard: :pencil2: :_***  As informa√ß√µes s√£o quebradas desta forma por uma quest√£o de utilidade.

![Aula01_Figura29](imagens/Aula01_Figura29.png)

Na imagem do n√∫mero `4`, o `primeiro fragmento` √© representado por uma linha vertical ‚Äî a mesmo vista no n√∫mero `9` ‚Äî, o `segundo fragmento` √© tamb√©m uma linha vertical e o `terceiro fragmento` representado por uma linha na horizontal.

O treinamento de reconhecimento destas imagens √© feito ao mesmo tempo nas camadas ocultas de mais alto n√≠vel.

> ***_NOTA :clipboard: :pencil2: :_*** As camadas ocultas de mais alto n√≠vel s√£o aquelas que est√£o mais pr√≥ximas da camada de sa√≠da.

![Aula01_Figura30](imagens/Aula01_Figura30.png)

Assim, obteremos o seguinte resultado:

![Aula01_Figura31](imagens/Aula01_Figura31.png)

Cada um dos neur√¥nios guarda a informa√ß√£o da representa√ß√£o de cada um dos desenhos que vimos.

A ativa√ß√£o dos neur√¥nios em verde nos mostra que h√° grandes chances do n√∫mero `4` ser ativado. J√° ativa√ß√£o dos neur√¥nios em roxo, maiores chances de ser o n√∫mero `8`.

> :key: :bulb: At√© aqui entendemos que a Rede neural coleta as informa√ß√µes que est√° observando, as combina, e toma alguma decis√£o a partir disso.

## Como cada peda√ßo de fragmento √© identificado?

<!-- 49:40 -->

Em m√©dia, um Ser Humano √© capaz de desenhar um c√≠rculo somente aos 3 anos de idade. Neste per√≠do, estamos aprendendo a coordenar o campo visual juntamente com a musculatura fina da m√£o-de-escrita para que o formato redondo saia no papel. √â uma tarefa complexa e desafiadora e o mesmo acontece no aprendizado da Rede Neural.

> **_Curiosidade :brain: :_** O c√≠rculo √© um s√≠mbolo universal com significado amplo, nos remete √†s no√ß√µes de totalidade, plenitude, perfei√ß√£o original, o Eu, o infinito, a eternidade. Interessante notar que uma das formas mais espalhadas na natureza exija tanto de n√≥s aprendermos seu formato. 

![Aula01_Figura32](imagens/Aula01_Figura32.png)

Note no exemplo abaixo, que o entendimento do que √© um c√≠rculo pela Rede Neural √© a jun√ß√£o de `5 fragmentos`.

![Aula01_Figura33](imagens/Aula01_Figura33.png)

Lembra que anteriormente mencionamos que o aprendizado de algumas imagens √© feito em uma camada de alto n√≠vel?

![Aula01_Figura34](imagens/Aula01_Figura34.png)

Na primeira camada de neur√¥nios do exemplo acima, a informa√ß√£o que j√° foi aprendida √© fragmentada em pequenos peda√ßos e somada √†s informa√ß√µes da segunda camada formando algo novo, como o formato do c√≠rculo que vimos no d√≠gito `9`. O mesmo acontece com a sua perninha, seu reconhecimento tamb√©m √© fragmentado pela Rede Neural:

![Aula01_Figura35](imagens/Aula01_Figura35.png)

Para fins did√°ticos, podemos dividir o processo de aprendizado do d√≠gito `9` em quatro partes:

- Na camada de entrada, a informa√ß√£o do d√≠gito `9` achatado √© recebida;
- Na primeira camada oculta, os fragmentos do d√≠gito `9` s√£o reconhecidos e ativados;
- Na segunda camada oculta, o d√≠gito `9` √© constru√≠do com apenas dois fragmentos ativando apenas dois neur√¥nios, e;
- Na camada de sa√≠da, o preenchimento do neur√¥nio correspondente ao d√≠gito `9` √© realizado.

![Aula01_Figura36](imagens/Aula01_Figura36.png)

> :key: :bulb: Lembre-se que cada neur√¥nio √© um par√¢metro multiplicado por peso e _bias_.

## Evoluindo a fun√ß√£o do neur√¥nio

At√© aqui tratamos o neur√¥nio como **algo que guarda um n√∫mero**. Ap√≥s tudo que estudamos, vimos que seu trabalho vai al√©m disso: **o neur√¥nio √© uma fun√ß√£o matem√°tica**.

![Aula01_Figura37](imagens/Aula01_Figura37.png)

Por ser uma fun√ß√£o, o neur√¥nio agora vai **receber** n√∫meros e **transform√°-los** tornando todo o processo de aprendizagem mais flex√≠vel.

Vamos revisar brevemente todas essas informa√ß√µes adicionando essa nova informa√ß√£o...

Recebemos o nosso neur√¥nio inicial com seus par√¢metros configurados. 

![Aula01_Figura38](imagens/Aula01_Figura38.png)

Para a nossa **primeira** camada oculta os par√¢metros s√£o a camada de entrada. 

- O **primeiro** pixel ser√° multiplicado pelo peso aleat√≥rio `1`.

![Aula01_Figura39](imagens/Aula01_Figura39.png)

> :key: :bulb: Neste momento vamos assumir que o valor do peso √© definido de forma **aleat√≥ria**.

- O **segundo** pixel ser√° multiplicado pelo peso aleat√≥rio `2`.

![Aula01_Figura40](imagens/Aula01_Figura40.png)

Por exemplo:

- Par√¢metro 1 x Peso 1: `1 x 0 = 0`
- Par√¢metro 2 x Peso 2: `1 x 5 = 5`

Tendo em m√£os o resultado de cada par√¢metro, o resultado final ser√°:

- `Total = 5`

![Aula01_Figura41](imagens/Aula01_Figura41.png)

> :key: :bulb: Para cada pixel entrando n√≥s repetimos sua multiplica√ß√£o pelo peso e o somamos aos valores obtidos dos pixels que entraram na camada anteriormente.

N√£o podemos esquecer de agregar o **valor de _bias_** que pode ser um valor negativo ou positivo, e que neste momento tamb√©m ser√° aleat√≥rio. 

- Par√¢metros + _bias_: `5 + 30 = 35`

![Aula01_Figura42](imagens/Aula01_Figura42.png)

> :key: :bulb: Os valores aleat√≥rios s√£o ajustados conforme a Rede Neural vai crescendo em aprendizado. 

Com o valor total da soma de par√¢metros + _bias_ em m√£os, precisamos pass√°-lo por uma **fun√ß√£o de ativa√ß√£o** que o transformar√° em outro resultado. 

Para este exemplo, esta fun√ß√£o **dobra** o valor recebido.

- Total anterior -> fun√ß√£o de ativa√ß√£o: `35 x 2 = 70`

![Aula01_Figura43](imagens/Aula01_Figura43.png)

> :key: :bulb: A forma de funcionamento de uma fun√ß√£o de ativa√ß√£o √© determinada por n√≥s. Al√©m de dobrar o valor em nosso exemplo, esta condi√ß√£o poderia valer somente para os n√∫meros pares.

O processo de **aprendizado da Rede Neural** pode ser entendido da atrav√©s de todas essas etapas que passamos. Lembre-se  :mega: :

```
- O neur√¥nio √© uma estrutura com uma fun√ß√£o de ativa√ß√£o;
- Pesos e bias inicialmente s√£o valores aleat√≥rios, e;
- O valor de cada peso e bias √© ajustado e com isso o aprendizado vai aumentando.
```

:key: :bulb: Em s√≠ntese, aprendizado em Rede Neural √© um constante ajustar de pesos e _bias_ focando sempre no resultado que queremos obter na camada de sa√≠da. 

![Aula01_Figura44](imagens/Aula01_Figura44.png)

## Determinando a Quantidade de Pesos em uma Rede Neural Profunda

<!-- 56:35 -->

Sabemos que o valor de cada pixel recebido pela camada de entrada t√™m seu par√¢metro multiplicado pelo peso e somado ao valor de _bias_.

Vamos usar o exemplo de uma Rede Neural que possui **16 neur√¥nios** em ambas camadas ocultas e **784 pixels** sendo recebidos na camada de entrada.

Cada neur√¥nio ter√° seu par√¢metro somado ao _bias_ na **primeira camada oculta**.

Cada neur√¥nio ter√° seu par√¢metro somado ao _bias_ na **segunda camada oculta**.

Este total ser√° multiplicado pelas `10` op√ß√µes dispostas na **camada de sa√≠da**, ou seja, os n√∫meros de `0 a 9` por se tratar dos valores que esperamos.

![Aula01_Figura45](imagens/Aula01_Figura45.png)

> :key: :bulb: O valor total deste c√°lculo resulta na **quantidade de pesos** desta Rede Neural Profunda. A depender da quantidade de camadas ocultas configuradas, teremos mais ou menos pesos.

## Fun√ß√µes em Rede Neural

<!-- 57:50 -->

Existem diversas fun√ß√µes √† nossa disposi√ß√£o para trabalharmos. Vamos come√ßar com uma bastante simples, sua nota√ß√£o matem√°tica √© a seguinte:

`f(x) = 2x`

Uma fun√ß√£o √© sempre algo que **transforma** um valor. Podemos imagin√°-la como uma caixa onde uma informa√ß√£o est√° entrando (**_Input_**), passa pelo corpo da caixa, e sua transforma√ß√£o √© devolvida na sa√≠da (**_Output_**).

![Aula01_Figura46](imagens/Aula01_Figura46.png)

Supondo que colocamos o valor de `10` na camada de entrada, ele √© multiplicado por `2`, resultando em `20` na camada de sa√≠da.

Se assumirmos que o valor de `x` ser√° `10`, onde `x` aparecer seu valor ser√° `10`.

- `f(x) = 2x`
- `f(x) = 2 x 10`
- f(x) = 20

A fun√ß√£o `f` recebendo o valor `(10)` na camada de entrada, ter√° o valor `20` na camada de sa√≠da.

Vamos supor que queremos `f(x)` para o intervalo de `-10 a 10`. Graficamente poderemos representar desta forma:

![Aula01_Figura47](imagens/Aula01_Figura47.png)

Note que o n√∫mero `1` do eixo **x** assume valor `2` em **y**.

Passando o intervalo de `-10 a 10` nesta fun√ß√£o, teremos como resultado o intervalo de `-20 a 20`.

> ***_NOTA :clipboard: :pencil2: :_*** Sempre podemos colocar uma fun√ß√£o em um gr√°fico para observar seu comportamento e dizer se ela nos retorna o que esperamos.

![Aula01_Figura48](imagens/Aula01_Figura48.png)

Para saber mais a respeito de fun√ß√µes e como funcionam, recomendamos a leitura no material do _Deep Learning Book_ - [Cap√≠tulo 8](https://www.deeplearningbook.com.br/funcao-de-ativacao/). 

### Escolhendo a Fun√ß√£o de Ativa√ß√£o Correta

<!-- Paulo, vale colar aqui no material a pincelada que voc√™ deu sobre a sugest√£o de leitura anterior a respeito destas fun√ß√µes e dar exemplos? -->

### Fun√ß√£o Sigmoide

<!-- 1:05:11 -->

Vamos novamente recorrer ao recurso visual para nos ajudar. No exemplo abaixo, temos um intervalo que vai de `-5 a 5`:

![Aula01_Figura49](imagens/Aula01_Figura49.png)

- Se o valor de **x** est√° entre `0 e 2` o valor de **y** estar√° entre `0 e 0.9`.

- Se o valor de **x** est√° entre `2 e 5` o valor de **y** estar√° entre `0.9 e 1`.

Podemos notar que quando **x** est√° pr√≥ximo a `0` o valor de **y** √© bastante influenciado e o mesmo n√£o acontece quando **x** se afasta de `0`.

O mesmo acontecer√° quando o intervalor de **x** estiver entre `0 e -2`.

> ***_NOTA :clipboard: :pencil2: :_*** A fun√ß√£o **sigmoide** tamb√©m transforma um n√∫mero. Se o valor de entrada for `2`, ela devolver√° `0.9` como valor de sa√≠da.

Abaixo, podemos ver a representa√ß√£o de um neur√¥nio funcionando com a fun√ß√£o sigmoide:

![Aula01_Figura50](imagens/Aula01_Figura50.png)

O valor de entrada `a1` √© multiplicado pelo peso `w1` e o mesmo acontece para a quantidade de valores que entram. Se estamos recebendo os `784px` de imagens, este processo acontecer√° de (`w1a1 + ... + w784a784`).

Ap√≥s o resultado de todas as somas, o valor de _bias_ √© aplicado. Para este exemplo, o valor de`-10` foi escolhido.

> ***_NOTA :clipboard: :pencil2: :_*** A complexidade dos c√°lculos matem√°ticos s√£o todos feitos pelo computador. A n√≥s, cabe somente escolher a fun√ß√£o que mais se adequa ao nosso prop√≥sito.

## Recapitulando

<!-- 1:07:10 a 1:16:00 - TRANSFORMAR ESTE PEDA√áO EM UM RESUMO (BULLETS) QUE ANTECEDE TUDO E COLOCAR COMO TEXTO DE CONSULTA NA TABELA DA P√ÅGINA INICIAL ? -->

<!-- 1:07:10 a 1:16:00 - TRANSFORMAR ESTE PEDA√áO EM UM RESUMO (BULLETS) E INSERIR NO COME√áO DO MATERIAL DO V√çDEO 02? -->

## Entendendo o Processo de Aprendizado de M√°quina

<!-- 1:16:13 -->

A ess√™ncia do treinamento em Redes Neurais √© o m√©todo de **_backpropagation_**. Um ajuste fino √© realizado nos pesos da Rede Neural base na **taxa de erros** obtidos durante o treinamento.

![Aula01_Figura51](imagens/Aula01_Figura51.png)

Adequadamente feitos os ajustes, as taxas de erro v√£o diminuindo tornando o modelo confi√°vel e aumentando sua capacidade de generalizar os resultados e nos devolvendo na camada de sa√≠da o que esperamos.

> ***_NOTA :clipboard: :pencil2: :_***  _Backpropagation_ √© o m√©todo padr√£o de treinamento em Redes Neurais.

Quando falamos em **erros** cometidos pela Rede Neural, buscamos entender o qu√£o longe estamos de acertar.

![Aula01_Figura52](imagens/Aula01_Figura52.png)

Vamos agora relembrar o ciclo de uma Rede Neural e para fixarmos adicionando a informa√ß√£o de _Backpropagation_.

- Elegemos uma **base de treino** para o nosso modelo e realizamos o processo de achatamento das imagens que queremos utilizar;
- As informa√ß√µes s√£o inseridas na **camada de entrada**;
- Os par√¢metros de **peso** s√£o aplicados;
- Do valor obtido √© ent√£o feito o **somat√≥rio**;
- O **bias** √© aplicado a estes par√¢metros;
- O valor resultante √© passado pela **fun√ß√£o de ativa√ß√£o**;
- A **previs√£o** extra√≠da da fun√ß√£o de ativa√ß√£o √© comparada ao **resultado esperado** atrav√©s de uma **fun√ß√£o de erro**;
- A fun√ß√£o de erro determina **a quantidade de erros**.

![Aula01_Figura53](imagens/Aula01_Figura53.png)

- Assim como acontece conosco quando analisamos e corrigimos nossos erros diante de um aprendizado para qualquer desafio, o mesmo acontece no aprendizado da m√°quina. **Cada erro nos aproxima do resultado esperado**.
- Saber o resultado que queremos nos permite fazer um **ajuste nos pesos**. Ap√≥s isso, refazemos o processo at√© chegar novamente na fun√ß√£o de erro avaliando se outro reajuste √© necess√°rio.

![Aula01_Figura54](imagens/Aula01_Figura54.png)

> ***_NOTA :clipboard: :pencil2: :_*** As previs√µes v√£o ficando cada vez mais assertivas conforme este ciclo vai se repetindo. Aprender -> Errar -> Ajustar -> (Re)aprender -> (Re)ajustar tamb√©m faz parte dos nossos ciclos de vida. :blue_heart:

Este ciclo que se repete at√© o aprendizado completo se chama **√©poca**.

Para cada imagem inserida na camada de entrada, uma √©poca √© realizada. Seu n√∫mero de repeti√ß√µes e o entendimento do que acontece individualmente em cada uma √© observado e configurado por n√≥s. Gra√ßas √† tecnologia, e aos que vieram antes, todo o processo matem√°tico feito em cada √©poca √© realizado por bibliotecas. :boom:

Voc√™ se lembra que o _Dataset_ do MNIST possui 60.000 imagens de treino? Cada uma dessas imagens passa por uma √©poca e paralelamente a compara√ß√£o com a **base de testes** √© feita para que os ajustes de pesos possam ser feitos. Assim, saberemos se a imagem reconhecida pela Rede e exposta na camada de sa√≠da √© a que inserimos no modelo.

![Aula01_Figura55](imagens/Aula01_Figura55.png)

O processo de compara√ß√£o com a base de testes √© extremamente importante, n√£o s√≥ para avaliar o que estamos fazendo mas tamb√©m para nos dizer sobre um problema chamado **_Overfitting_**.

O _overfitting_ acontece quando estamos acertando muito na base de treino e errando na mesma propor√ß√£o na base de testes, ou seja, quando estamos exagerando no polimento dos par√¢metros.

![Aula01_Figura56](imagens/Aula01_Figura56.png)

Pode acontecer de nos preocuparmos demasiadamente em ensinar nosso algoritmo a reconhecer a base de treino recebida e n√£o em generalizar e focar na resolu√ß√£o do problema para que quando novas informa√ß√µes entrem, sejam identificadas.

> :key: :bulb: √â de extrema import√¢ncia acertar na base de treino. Contudo, tamb√©m √© importante acertar na base de testes para que o algoritmo seja capaz de reconhecer no dia-a-dia imagens novas e nunca vistas. 
>
> Caso voc√™ se perca no processo, lembre-se de recordar o que est√° fazendo e o por qu√™. :wink:

## Entendendo a Fun√ß√£o de Erro

<!-- 1:26:53 -->

Novamente, vamos usar um gr√°fico para nos ajudar a entender o funcionamento de uma fun√ß√£o de erro. 

A linha de cor `laranja` possui pontos de cor `azul` espalhados que acompanham seu tra√ßado. Cada ponto azul possui uma dist√¢ncia entre sua **borda** e o **ponto que ele toca** a reta laranja naquele momento do espa√ßo.

![Aula01_Figura59](imagens/Aula01_Figura59.png)

Para definirmos a quantidade de erros e acertos necessitamos separar cada uma das retas e somar suas diferen√ßas, ou seja:

- r1 = possui 2 unidades
- r2 = possui 2 unidades
- r3 = possui 1 unidade
- r4 = possui 1 unidade
- r5 = possui 1 unidade

> ***_NOTA :clipboard: :pencil2: :_*** Neste momento, o valor de cada ponto (r) n√£o interfere no resultado pois representa apenas uma unidade.

A soma total √© de `7` unidades, ou seja, o **erro total foi de 7 unidades**.

![Aula01_Figura58](imagens/Aula01_Figura58.png)

Utilizando-nos da matem√°tica percebemos que ao percorrer a linha laranja possu√≠mos somente **um √∫nico acerto**, pois `-2` anula `2` e `-1` anula `1` restando somente o `1` ao final da reta.

![Aula01_Figura57](imagens/Aula01_Figura57.png)

Pois bem,  algoritmo de Rede Neural n√£o trabalha com n√∫meros negativos e por este motivo as **fun√ß√µes de erro geralmente consideram a soma dos erros quadrados**, ou seja, todos os valores negativos :cry: s√£o **elevados ao quadrado** para que se tornem positivos :happy:.

> ***_Importante :mega: :_*** Quanto maior a barra de erros, ou seja, mais longe de `0`, mais erros cometemos ao configurar o algoritmo. 

> ***_Importante :mega: :_***Quanto mais pr√≥ximo de `0`, menos erros cometemos ao configurar o algoritmo. No nosso exemplo erramos bastante pois `7` est√° bastante longe de `0`.

## _Utter Trash_? _Utter_ Quem?

Observando a figura abaixo voc√™ conseguiria dizer o que est√° acontecendo na camada de sa√≠da e a expectativa do que era esperado?

![Aula01_Figura60](imagens/Aula01_Figura60.png)

Podemos observar que a camada de sa√≠da do modelo possui diversos neur√¥nios bastante cheios, tendo o do n√∫mero `8` se destacando. Entretanto, o resultado esperado √© o n√∫mero `3`.

>  ***_NOTA :clipboard: :pencil2: :_*** O "custo" da diferen√ßa entre o resultado esperado e o resultado na camada de _output_ se chamada **_Utter Trash_**.

Sugerimos que d√™ uma pausa aqui e assista o terceiro v√≠deo da 3Blue1Brown [neste link](https://www.3blue1brown.com/lessons/neural-network-analysis) para que veja o processo de _Utter Trash_ acontecendo e o compreenda melhor.

## As Par√°bolas que as Fun√ß√µes de Erro nos Conta

Uma fun√ß√£o de erro normalmente resulta em uma par√°bola, nos possibilitando encontrar seu ponto mais baixo, ou seja, mais pr√≥ximo a `0`.

Obtendo este menor valor que n√£o `0` ainda, conseguimos minimizar e ajustar a fun√ß√£o aproximando-a de `0`, que √© o nosso objetivo final.

![Aula01_Figura61](imagens/Aula01_Figura61.png)

A par√°bola acima n√£o √© est√° no formato que normalmente encontramos em Redes Neurais, e sim, um formato que se assemelha a **vales**. 

Igualmente √† par√°bolda, em cada vale buscamos aproxim√°-lo de `0`.

![Aula01_Figura62](imagens/Aula01_Figura62.png)

A bolinha amarela acima vai caminhando pelos vales de acordo com os ajustes que vamos realizando nos pesos conforme as √©pocas v√£o sendo executadas. A camada de sa√≠da da Rede nos mostrar√° se devemos ajustar o peso para **mais** ou para **menos**.

> ***_Importante :mega: :_*** Sempre, **o vale mais adequado para o nosso objetivo √© o que mais pr√≥ximo est√° de `0`**. Encontrar o ponto √≥timo, ou seja, nos aproximarmos de `0` √© o processo do aprendizado da rede em si.

Sugerimos que d√™ uma pausa aqui e assista o segundo v√≠deo da 3Blue1Brown [neste link](https://www.3blue1brown.com/lessons/gradient-descent) para que este conte√∫do fique mais claro em sua mente.

## TensorFlow - Nosso Lugar para Experimentar em Redes Neurais 

Ao acessar o site do [TensorFlow Playground](https://playground.tensorflow.org/) nos deparamos com uma estrutura de Rede Neural.

![Aula01_Figura63](imagens/Aula01_Figura63.png)

As **_features_** consistem nas propriedades que vamos alimentar a Rede.

No exemplo acima, possu√≠mos `2` camadas escondidas, a **primeira** com `4` neur√¥nios e a **segunda** com apenas `2`.

>  Tanto a quantidade de camadas ocultas quanto a de neur√¥nios pode ser modificada.

No canal **_Data_** escolhemos o _dataset_ que iremos trabalhar.

Podemos observar na camada de sa√≠da diversas bolinhas `azuis` ao centro cercadas de diversas bolinhas `laranjas`. Neste caso, o que a Rede precisa entender √© que:

- Azul representa `informa√ß√£o` com valor `1`, e;
- Laranja representa `informa√ß√£o` com valor `-1`.

Quando apertamos o _Play_, dizemos √† rede para executar as √©pocas e iniciar o aprendizado. Note como ela est√° se comportando na **√©poca 340**.

![Aula01_Figura64](imagens/Aula01_Figura64.png)

Na camada de sa√≠da fica evidente que a Rede est√° se ajustando e aprendendo cada vez mais a diferenciar o que √© bolinha azul do que √© bolinha laranja. Contudo, seu aprendizado ainda n√£o est√° completo.

Quando a base de treino atinge o valor `0.001`, neste exemplo, uma estagna√ß√£o foi atingida e nenhum aprendizado est√° sendo somado ao processo.

> ***_Importante :mega: :_*** Se dermos o _Play_ diversas vezes notaremos que a camada de sa√≠da apresentar√° um comportamento diferente por conta do refinamento no ajuste dos pesos, que de in√≠cio, s√£o aleat√≥rios.

## Desafio Proposto

Nosso desafio para voc√™ hoje √© configurar a Rede para aprender o problema **espiral** que est√° incluso no _dataset_ do site.



Obrigado(a) at√© aqui e at√© a volta, nos vemos em breve! :blue_heart:

## ‚ùóÔ∏è Links & Refer√™ncias usados nesta aula

Esta aula no <a href="https://miro.com/app/board/o9J_ljr0G-g=/" target="_blank">Miro</a>

Site <a href="https://playground.tensorflow.org/" target="_blank">Tensorflow Playground</a>

Site <a href="https://www.3blue1brown.com/topics/neural-networks" target="_blank">3Blue1Brown</a>



## <!-- Pend√™ncias -->

<!-- CHANELY - CRIAR UM DICION√ÅRIO-R√ÅPIDO PARA OS TERMOS: (SALVATORE VALIDAR) -->

<!-- NEURONIO -->

<!-- CONEX√ïES -->

<!-- BIAS -->

<!-- FUN√á√ÉO DE ATIVA√á√ÉO -->

<!-- CAMADA DE ENTRADA -->

<!-- CAMADA OCULTA -->

<!-- CAMADA DE SA√çDA -->

<!-- FORMATO DE ENTRADA -->

<!-- PESOS -->

<!-- PROPAGA√á√ÉO -->

<!-- BACKPROPAGATION -->

<!-- TAXA DE APRENDIZAGEM -->

<!-- PRECIS√ÉO -->

<!-- ACUR√ÅCIA -->

<!-- SENSIBILIDADE -->

<!-- CONVERGENCIA -->

<!-- REGULARIZA√á√ÉO -->

<!-- NORMALIZA√á√ÉO -->

<!-- CAMADAS COMPLETAMENTE CONECTADAS -->

<!-- PERDA DE FUN√á√ÉO -->

<!-- OTIMIZA√á√ÉO DE MODELOS -->

<!-- METRICAS DE PERFORMANCE -->

<!-- BATCH SIZE -->

<!-- TRAINING EPOCHS -->
