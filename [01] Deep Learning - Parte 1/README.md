# Aula #1 - Deep Learning - Parte 1

Por <a href="https://www.youtube.com/paulosalvatore" target="_blank">Paulo¬†Salvatore</a> - Head de Produtos <a href="https://blueedtech.com.br/quem-somos/" target="_blank">@Blue¬†Edtech</a> & Chanely Marques - Eterna Aprendiz <a href="https://blueedtech.com.br/quem-somos/" target="_blank">@Blue Edtech</a>

Ol√° :wave:, seja bem-vindo(a) ao nosso primeiro v√≠deo desta s√©rie. Nosso objetivo √© introduzir o conceito de rede neural, neur√¥nios, aprendizagem profunda, fun√ß√µes de ativa√ß√£o, algoritmo de Backpropagation e aplica√ß√£o pr√°tica com TensorFlow Playground.

## Introdu√ß√£o

Antes de mergulharmos no **Deep Learning** (D.L. - aprendizagem profunda) √© necess√°rio sabermos que ele est√° dentro de uma estrutura de conhecimentos da **Intelig√™ncia Artificial** (I.A.).

![Aula01_Figura01](imagens/Aula01_Figura01.png)

A I.A. come√ßou sua evolu√ß√£o pela d√©cada de 50 e faz parte do campo de estudos da Ci√™ncia da Computa√ß√£o que se combina a conjuntos de dados robustos para permitir a resolu√ß√£o de problemas. Alan Turing, muitas vezes chamado de "pai da ci√™ncia da computa√ß√£o", frequentemente questionava se as m√°quinas seriam capazes de pensar. A partir da√≠ definiu que a I.A. seria categorizada como <a href="https://www.ibm.com/br-pt/cloud/learn/what-is-artificial-intelligence" target="_blank">"sistemas que agem como humanos"</a>.

O **Machine Learning** (M.L. - aprendizagem de m√°quina) veio um pouquinho depois, se tornando uma t√©cnica de aprendizagem de m√°quina para que ela reconhe√ßa padr√µes e aprenda em cima de um grande conjunto de dados e a partir deste aprendizado tomamos decis√µes, fazemos previs√µes, entre outros.

O Deep Learning fez sua primeira apari√ß√£o entre as d√©cadas de 70 e 80 e come√ßou a se popularizar a partir de 2010. √â um m√©todo que incorpora redes neurais em camadas para que a aprendizagem a partir dos dados seja feita de forma iterativa.

![Aula01_Figura02](imagens/Aula01_Figura02.png)

Tanto M.L. quanto o D.L recebem informa√ß√µes na camada de **Input** (entrada) e, baseados nestas informa√ß√µes, tomam decis√µes ou fazem alguma previs√£o na camada de **Output** (sa√≠da), comparando aos dados que j√° eram conhecidos.

No processo de M.L., os dados entram pela camada de **entrada**, e a extra√ß√£o deles √© feita completamente por um Ser Humano. Estruturados e organizados, estes dados s√£o classificados por uma m√°quina, gerando um resultado na camada de **sa√≠da**.

> ***_NOTA :clipboard: :pencil2: :_***  O processo de extra√ß√£o se caracteriza por observar, analisar e estruturar o conjunto de dados recebidos na camada **de entrada**.

> ***_NOTA :clipboard: :pencil2: :_***  O processo de classifica√ß√£o √© feito inteiramente por uma m√°quina, atrav√©s de uma ou um conjunto de f√≥rmulas matem√°ticas classificando todas as features extra√≠das.

No processo de D.L., os dados entram pela camada de **entrada**, e os processos de extra√ß√£o e classifica√ß√£o das features, s√£o feitos inteiramente pela m√°quina gerando a **sa√≠da**. A grande vantagem √© que ele pode nos trazer informa√ß√µes interessantes e/ou relevantes, que poderiam passar despercebidas pelo Ser Humano.

> **_Obs.:_** Para ambos os processos, os dados precisam ser preparados por um Ser Humano para entrar no processo de aprendizagem.

Para que estes conceitos fiquem mais claros, assista aos v√≠deos a seguir:

- Este carro aut√¥nomo usa o D.L. para saber se objetos que passam no campo de vis√£o s√£o carros ou n√£o: <a href="https://www.youtube.com/watch?v=mUV5ZwIC9_g&feature=youtu.be" target="_blank">Carro¬†Aut√¥nomo</a>.

- No reconhecimento de voz as palavras s√£o identificadas atrav√©s da inser√ß√£o de v√°rios arquivos de voz: <a href="https://www.youtube.com/watch?v=NaqZkV_fBIM&feature=youtu.be" target="_blank">Reconhecimento¬†de¬†Voz</a>.

- Projeto GauGAN da Nvidia que transforma desenhos em paisagens usando Redes Neurais: <a href="https://www.youtube.com/watch?v=p5U4NgVGAwg" target="_blank">GauGAN</a>.

- Poder de an√°lise de dados entre CPU vs. GPU: <a href="https://www.youtube.com/watch?v=-P28LKWTzrI" target="_blank">CPU¬†vs. GPU</a>.

### Formas de An√°lise de Inser√ß√£o de Dados

Neste exemplo, somos um algoritmo de I.A. com o objetivo de entender informa√ß√µes:

![Aula01_Figura03](imagens/Aula01_Figura03.png)

A letra `A` pode ser entendida de diversas formas:

- Ao analisar sua imagem, independente da forma que foi escrita
  - No caso do computador, uma imagem √© um conjunto de pixels
  - No caso do ser humano, uma imagem √© gerada pelos nossos olhos e processada pelo c√©rebro
- Ouvindo-a sendo dita
  - No caso do computador, um √°udio √© um arquivo que cont√©m v√°rias faixas de frequ√™ncia (medidas em Hertz) e intensidades diferentes para cada valor de frequ√™ncia
  - No caso do ser humano, um som √© interpretado por nossos ouvidos e processado pelo c√©rebro

Para que, ao final do processo, tenhamos a informa√ß√£o que aquela letra representa, precisamos transform√°-la, pois, afinal, n√£o √© t√£o pr√°tico usar um conjunto de pixels ou um arquivo de √°udio em um programa de computador, trabalhar diretamente com a informa√ß√£o √© muito melhor e mais pr√°tico.

O mesmo processo √© feito para d√≠gitos num√©ricos:

![Aula01_Figura04](imagens/Aula01_Figura04.png)

O d√≠gito pode ter sido escrito √† m√£o, digitado em um computador ou extra√≠do de uma imagem. Como algoritmos, precisamos ler estes **dados** para saber que o d√≠gito √© um `5` e n√£o uma representa√ß√£o dele.

Este mesmo processo de `Entrada de dados -> An√°lise -> Sa√≠da` √© feito pelo nosso c√©rebro o tempo todo. Atrav√©s da rede neural, espalhada por todo o corpo √© captamos as informa√ß√µes do ambiente, as transformamos em est√≠mulos sensoriais que s√£o entendidos e traduzidos pelos neur√¥nios no c√©rebro que  nos devolve a informa√ß√£o em forma de sensa√ß√£o de tato, audi√ß√£o, vis√£o ou paladar e a partir disso tomamos alguma decis√£o.

## Prepara√ß√£o da Rede Neural

<!-- 13:08 -->

Para que o c√©rebro consiga de fazer todas as coisas que hoje sabemos que √© capaz de fazer, podemos imaginar que um √∫nico neur√¥nio t√™m sua estrutura e forma de funcionar bastante complexa. 

Por hora n√£o vamos nos servir desta complexidade, mas sim de sua simplicidade funcional: receber e transmitir impulsos el√©tricos, ou seja, receber e transmitir informa√ß√µes.

![Aula01_Figura06](imagens/Aula01_Figura06.png)

A estrutura b√°sica de um neur√¥nio consiste em:

- **Dendritos**: √© por aqui que as informa√ß√µes entram - `Input`,
- **Corpo celular**: recebidas as informa√ß√µes, as mesmas ser√£o analisadas, processadas e decididas quanto ao destino,
- **Ax√¥nio envolto de ilhas de bainha de mielina**: fio condutor respons√°vel por transmitir as informa√ß√µes do corpo celular para o ax√¥nio terminal, e;
- **Ax√¥nio terminal**:  √© por aqui que as informa√ß√µes saem - `Output`.

Para que as informa√ß√µes saiam do ax√¥nio terminal `A` para o dendrito do ax√¥nio `B`, uma estrutura chamada **sinapse** ‚Äî que pode ser qu√≠mica ou el√©trica ‚Äî √© ativada atrav√©s do est√≠mulo que sai do ax√¥nio terminal de `A` para o dendrito de `B`.

Gra√ßas √† essas sinapses, nosso c√©rebro √© capaz de processar 11 bilh√µes de bits por segundo atrav√©s de conex√µes paralelas entre todos os neur√¥nios.

> ***_Curiosidade_*** üß†: Uma sinapse se forma entre um neur√¥nio do c√©rebro com outro neur√¥nio do c√©rebro. Quando um neur√¥nio do c√©rebro precisa falar com uma c√©lula muscular para ativar qualquer m√∫sculo no corpo, a estrutura se chama **jun√ß√£o neuromuscular**.

Este conceito de processar informa√ß√µes paralelamente √© bastante aplicado √† computa√ß√£o. Chamado de **processamento paralelo**, ele explora e usa simultaneamente v√°rias unidades de processamento (CPU's) para aumentar a velocidade.

Por exemplo, ao ouvirmos a letra `A`, a informa√ß√£o do som (em Hz) √© transmitida pelos condutos auditivos e captada pelos neur√¥nios especializados. Estes captadores formam uma cascata de est√≠mulos el√©tricos ativando o processamento paralelo nas √°reas cerebrais especializadas em audi√ß√£o, para que finalmente o reconhecimento da informa√ß√£o `A` seja feito e devolvido para n√≥s em forma de "Hmm, isso que eu ouvi √© a letra `A`, eu a conhe√ßo!".

Este mesmo processo √© feito para todas as informa√ß√µes que captamos do ambiente atrav√©s dos 5 sentidos.

> Por hora, queremos que voc√™ guarde o seguinte: podemos fazer uma analogia de tudo que vimos acima a um √∫nico neur√¥nio de m√°quina. Na m√°quina. este neur√¥nio realiza uma fun√ß√£o muito simples que √© a de armazenar um n√∫mero que, a princ√≠pio, ser√° entre 0 e 1.

![Aula01_Figura05](imagens/Aula01_Figura05.png)

## Representa√ß√£o Matem√°tica

<!-- 16:59 -->

Vamos observar e estudar com calma a figura abaixo:

![Aula01_Figura07](imagens/Aula01_Figura07.png)

Paralelamente √† estrutura funcional de um neur√¥nio, esta representa√ß√£o matem√°tica possui uma regi√£o que recebe **valores de entrada** (equivalente aos dendritos), outra que **analisa, processa e envia as informa√ß√µes** (equivalente ao corpo celular e ax√¥nio) e, por fim, a que faz a **sa√≠da** (equivalente ao ax√¥nio terminal).

Vamos trabalhar novamente o exemplo da letra `A`:

- a informa√ß√£o entrar√° na camada Roxa de entrada;
- ser√° processada na camada Laranja; e
- sair√° pela camada Verde de sa√≠da nos trazendo o resultado se a letra `A` foi ou n√£o identificada.

> ***Importante :bangbang: :*** Neste momento precisamos ter muito claro que a camada de entrada √© por onde entram as informa√ß√µes e a camada de sa√≠da √© por onde elas saem.

### Processamento das informa√ß√µes

Dentre todos os conceitos de uma rede neural, certamente o entendimento sobre **pesos (weights)** e **valores propagados (bias)** s√£o fundamentais (talvez a parte mais importante da uma rede neural üôÇ).

Quando a informa√ß√£o √© transmitida da camada de entrada para a de processamento, os pesos s√£o aplicados √† esta informa√ß√£o, somados e passados adiante para uma fun√ß√£o de ativa√ß√£o, juntamente com os valores de Bias.

Cada **informa√ß√£o que entra** pela camada de entrada √© **multiplicada** por um **peso** (que inicialmente corresponde um valor aleat√≥rio) antes de avan√ßar pela rede neural.

![Aula01_Figura08](imagens/Aula01_Figura08.png)

Ap√≥s essa m√∫ltiplica√ß√£o da **informa√ß√£o que entra** vezes o **peso**, aplicamos em um **somat√≥rio**, que representa a soma de todos os valores.

Por exemplo, vamos imaginar que recebemos na camada de entrada as letras `A` e `B`, cada uma com o seguinte valor:

- `Letra A` = `2`
- `Letra B` = `5`

Para cada letra, recebemos tamb√©m um **peso**, com os seguintes valores:

- `Peso A` = `3`
- `Peso B` = `6`

Com isso, temos os seguinte c√°lculo para saber o valor do **somat√≥rio**:

- `(Letra A * Peso A) + (Letra B * Peso B)`

Substituindo pelos valores:

- `(2 * 3) + (5 * 6)` que √© igual a `36`

:key: :bulb: Portanto, o valor do **somat√≥rio** √© `36`.

 Agora, podemos somar um valor de **bias** ‚Äî que pode ser um n√∫mero positivo ou negativo ‚Äî e nesse caso ser√° `-2`.

- `Somat√≥rio + Bias`
- `36 + -2` √© igual a `34`

:key: :bulb: Portanto, o resultado corresponde a `34`.

Com o resultado total em m√£os, ele ser√° passado por uma fun√ß√£o de ativa√ß√£o e, a partir dela, teremos um novo n√∫mero que ser√° passado para a camada de sa√≠da.

Vamos entrar mais a fundo em cada uma dessas partes?

<!-- 21:55 -->

## Dataset MNIST

Para aprofundarmos nosso estudo e deix√°-lo mais tang√≠vel, vamos falar sobre um banco de dados bastante espec√≠fico chamado **MNIST** que cont√©m imagens bin√°rias de d√≠gitos escritos √† m√£o por diversas pessoas.

![Aula01_Figura09](imagens/Aula01_Figura09.png)

> ***Importante*** :bangbang:: Desenvolver uma rede neural capaz de compreender os d√≠gitos destas imagens √© considerado o primeiro exemplo a ser aprendido por quem quer entender o universo, equivalente ao `Hello World!` no universo da programa√ß√£o. (PS: atente-se √† maldi√ß√£o :jack_o_lantern:)

O MNIST se tornou uma estrutura bem difundida entre a comunidade Deep Learning. Diversos outros projetos seguem a mesma base de imagens em escala de cinza, de tamanho 28x28 pixels, com 60.000 arquivos para treino e 10.000 arquivos para teste, organizadas em 10 classes distintas.

No caso do MNIST, cada classe distinta representa um dos n√∫meros de 0 a 9.

### Fashion MNIST

O **[Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)**, um banco de dados de imagens que segue a estrutura do MNIST, possui imagens de pe√ßas de roupa, tamb√©m organizadas em 10 classes distintas.

![Aula01_Figura10](imagens/Aula01_Figura10.png)

No v√≠deo-exemplo abaixo, podemos analisar como um algoritmo observa um conjunto de dados com artigos de moda em geral e os posiciona por similaridade.

![Aula1_Gif01](imagens/Aula01_Gif01.gif)

## E como interpretamos as imagens?

<!-- 23:57 -->

Das 70.000 imagens que podemos encontrar no MNIST, vamos pegar uma delas para o nosso exerc√≠cio. Essa imagem possui um tamanho de `28px` de largura por 28px de altura, totalizando `784px`, organizados em linhas e colunas, da seguinte forma:

![Aula01_Figura11](imagens/Aula01_Figura11.png)

Cada pixel na imagem √© an√°logo a um neur√¥nio, contendo n√∫meros que v√£o do intervalo de 0 a 1.

![Aula01_Figura12](imagens/Aula01_Figura12.png)

Quanto mais pr√≥ximo o neur√¥nio estiver do n√∫mero `1`, significa que ele est√° sendo mais ativado. Observe na imagem que a cor fica mais branca o n√∫mero `1.0` dentro dele, enquanto os outros pontos est√£o mais claros (entre `0.01` e `0.99`) ou at√© mesmo completamente pretos (`0.0`).

> ***_NOTA :clipboard: :pencil2: :_***  No dataset do MNIST, todas as imagens acompanham o significado do que ela representa. No nosso exemplo, a imagem recebe o significado `3`.

## E quando n√£o temos imagens?

Para uma an√°lise em que a camada de entrada receba um arquivo de √°udio, a an√°lise ser√° realizada da mesma forma observando a frequ√™ncia e sua intensidade.

![Aula01_Figura13](imagens/Aula01_Figura13.png)

Cada frequ√™ncia carrega consigo uma intensidade, gerando uma sa√≠da que pode ser uma letra qualquer ou uma frase, por exemplo.

## Modelo de Rede Neural

<!-- 27:47 -->

Semelhante √† representa√ß√£o do neur√¥nio, o modelo de rede neural carrega a mesma estrutura.

![Aula01_Figura14](imagens/Aula01_Figura14.png)

- **Camada de entrada (Input):** √© a primeira camada da rede neural que recebe os valores de entrada.

  No nosso exemplo temos os valores `1`, `0.5`e `0.2` entrando no modelo.

- **Camada Oculta (Hidden):** nesta camada encontramos os neur√¥nios organizados em n√≥s, interconectados com as camadas de entrada e sa√≠da, que aplicam os pesos e bias aos valores recebidos. De forma did√°tica e para nosso entendimento, podemos ver que estes n√≥s est√£o verticalmente empilhados e correspondem a `4` bolinhas azuis.

  > ***_NOTA :clipboard: :pencil2: :_***  √© nesta camada em que a rede realmente identifica o que est√° sendo recebido.

  O primeiro neur√¥nio da camada oculta recebeu os valores da camada de entrada, aplicou os pesos e o bias, alterando-os para o resultado `0.42`, entregando esse valor para a camada de sa√≠da.

- **Camada de sa√≠da (Output):** sendo a √∫ltima camada na rede, recebe o total da camada oculta.

  No nosso exemplo, o valor de sa√≠da modificado √© de `0.74`.

### Olha s√≥ que interessante! :boom:

Um modelo de rede neural que n√£o necessariamente est√° dentro do D.L., mas sim no M.L.

Quando vemos um modelo de rede neural com apenas **uma √∫nica camada oculta** estamos olhando para um modelo tradicional de M.L., que usamos em diversos exemplos.

A m√°gica fica maior, complexa e interessante quando adicionamos **mais camadas ocultas** a este modelo, caracterizando uma **Rede Neural Profunda**.

![Aula01_Figura15](imagens/Aula01_Figura15.png)

Logo, a quantidade de camadas ocultas √© o que define se meu modelo se trata de uma rede neural profunda ou apenas uma rede neural normal.

![Aula01_Figura16](imagens/Aula01_Figura16.png)

## Conectando tudo

<!-- 30:31 -->

Voc√™ se lembra que falamos sobre o MNIST alguns passos atr√°s? 

![Aula01_Figura09](imagens/Aula01_Figura09.png)

O banco de dados do MNIST possui um conjunto de treinamento de 60.000 imagens e um conjunto de testes com 10.000, ou seja, um sub-conjunto de um conjunto maior de treinamento dispon√≠vel. 

N√≥s utilizamos o sub-conjunto de 10.000 imagens-teste para comparar o resultado que vamos obtendo durante o treinamento e refinando o algoritmo.

Primeiro, apresentamos as imagens de **treino** ao modelo para que ela aprenda a reconhecer os padr√µes. Com isso, medimos os resultados.

Para garantir que os resultados s√£o coerentes, apresentamos as imagens de **teste** e verificamos se os resultados s√£o t√£o satisfat√≥rios quanto.

> Podemos fazer uma analogia ao treino de caligrafia (pessoa idosa aqui :older_woman:) que faz√≠amos na escola. At√© alcan√ßarmos algo parecido com a letra da professora (√© a meta que a gente almeja, n√©?! kkkrying) treinamos diversas vezes e, ao longo do processo, observamos em nosso caderno centenas de formas diferentes de fazer a mesma letra (todas um horror at√© a perfei√ß√£o, a gente sabe! :sweat:).

Na atividade **treina-compara-testa**, nossa rede √© capaz de generalizar, abstrair e observar novas imagens, garantindo que os resultados se tornam cada vez mais positivos (algo que pode ser medido na curva de aprendizado de uma rede neural).

Na pr√°tica, a imagem que a rede recebe do MNIST est√° em em preto e branco, normalizada por tamanho e centralizada para caber em uma caixa delimitadora de pixels de `28x28`, totalizando `784px`, e suaviza√ß√£o de borda criando os tons de cinza que vimos anteriormente.

Dentro do dataset, organizamos os arquivos das imagens (`Images`) em um local e a representa√ß√£o das imagens (`Labels`) em outro.

![Aula01_Figura17](imagens/Aula01_Figura17.png)

Ap√≥s escolhermos a imagem de treino para o nosso modelo, precisamos realizar um procedimento antes de a colocarmos na camada de entrada, este processo se chama **achatamento**.

Este processo consiste em transformar todo a matriz de `28x28px` em um vetor linear unidimensional, ou seja, em uma √∫nica linha, para conect√°-lo com a pr√≥xima camada.

![Aula01_Figura18](imagens/Aula01_Figura18.png)

As linhas achatadas s√£o feitas de forma sequencial, respeitando a ordem das linhas na matriz, ou seja, a primeira linha com 28 colunas √© achatada e colocada na primeira posi√ß√£o, a segunda tem suas 28 colunas achatas e colocadas na segunda posi√ß√£o, e assim por diante, gerando uma √∫nica linha com 784 pixels.

Cada um destes pixels √© o que de fato a rede vai usar para definir a representa√ß√£o da imagem.

Cada pixel da imagem contendo um valor que vai `0` a `1` equivale a um neur√¥nio na camada de entrada.

![Aula01_Figura19](imagens/Aula01_Figura19.png)

Podemos ver que o primeiro pixel vai para o primeiro neur√¥nio da camada e o √∫ltimo para seu neur√¥nio respectivo. Desta forma, temos todos os pixels da matriz representados dentro do modelo.

No nosso modelo de exemplo temos 2 camadas ocultas com 16 neur√¥nios cada.

> ***_NOTA :clipboard: :pencil2: :_***  A quantidade de camadas ocultas e de neur√¥nios s√£o definidas por n√≥s e existem v√°rias t√©cnicas para saber quando adicionar/remover camadas e neur√¥nios.

![Aula01_Figura20](imagens/Aula01_Figura20.png)

## Camada de Sa√≠da (Output)

Tudo que foi processado e se tornou relevante pelo nosso modelo deve ser apresentado nesta camada.

![Aula01_Figura21](imagens/Aula01_Figura21.png)

Em um outro exemplo, a nossa imagem de entrada √© a representa√ß√£o do n√∫mero `7`. Ela foi achatada e processada pelo modelo enchendo o neur√¥nio do n√∫mero `7` na camada de sa√≠da.

Se observarmos a figura acima atentamente perceberemos que temos um neur√¥nio para cada n√∫mero escrito √† m√£o contido dentro do Dataset.

> ***Importante*** :bangbang:: Identificar o d√≠gito de qualquer uma destas imagens pelo nosso Deep Learning √© uma tarefa an√°loga a um 'Hello World!'. Atente-se √† maldi√ß√£o. Cada possibilidade na camada de sa√≠da deve ser igualmente proporcional √† quantidade de itens que ser√£o inseridos atrav√©s de representa√ß√µes em nosso modelo.

Quando juntamos todas as camadas de nosso modelo percebemos que a entrada e sa√≠da est√£o sempre conectadas e relacionadas pelo conjunto de dados.



![Aula01_Figura22](imagens/Aula01_Figura22.png)

Para sabermos se nossa rede acertou o n√∫mero que inserimos, precisamos identificar o n√∫mero que ela assumiu para cada uma das informa√ß√µes e, a partir disso, vemos se acertou mais ou menos.

Os dados na camada de sa√≠da s√£o fundamentais para sabermos se a rede est√° acertando ou errando durante o treinamento. Lembrando que, quanto mais o valor do neur√¥nio estiver pr√≥ximo de `1`, mais certo est√° o resultado.

![Aula01_Figura23](imagens/Aula01_Figura23.png)

## Par√¢metros Utilizados - Pesos e Bias

<!-- 39:24 -->

Cada um dos 16 neur√¥nios da camada escondida est√° conectado a um pixel recebido da camada de entrada, ou seja, cada neur√¥nio recebe os 784 pixels.

![Aula01_Figura24](imagens/Aula01_Figura24.png)

Cada **peso** multiplicado ao **valor de entrada + bias** representa a for√ßa da conex√£o entre os neur√¥nios. Se o peso do neur√¥nio `1` ao `3` for maior que o peso do neur√¥nio `4` ao `7`, ter√° maior influ√™ncia sobre estes.

Por l√≥gica, percebemos que os par√¢metros podem diminuir a **import√¢ncia** dos pixels recebidos na camada de entrada e que ser√£o entregues na camada de sa√≠da, dependendo de sua configura√ß√£o na camada oculta.

![Aula01_Figura25](imagens/Aula01_Figura25.png)

Vamos agora a um exemplo pr√°tico e simples ‚Äî e que vai quebrar a tradi√ß√£o do 'Hello World' (certamente seremos perdoados pelos fins did√°ticos!).

Neste exemplo temos um √∫nico pixel com **duas** possibilidade de cores.

A **primeira** possibilidade representa o pixel de cor **preta**.

A **segunda** possibilidade representa o pixel de cor **branca**.

Note que a **camada de entrada** tem um √∫nico ponto e a de **sa√≠da** dois pontos, correspondentes ao preto e ao branco.

![Aula01_Figura26](imagens/Aula01_Figura26.png)

Quanto mais pr√≥ximo de `1`, mais o neur√¥nio de cor preta foi ativado na camada de sa√≠da.

Quanto mais pr√≥ximo de `1`, mais o neur√¥nio de cor branca foi ativado na camada de sa√≠da.

Sim! Existe a possibilidade dos neur√¥nios serem ativados juntos üòÆ!

Recebemos ent√£o, uma imagem com um pixel preto. Chamaremos esse √∫nico pixel de **um par√¢metro**.

Esse **par√¢metro de entrada** est√° conectado aos neur√¥nios na camada oculta. A esta conex√£o, damos o nome de **peso**.

> Para lembrar: Para cada um dos neur√¥nios, multiplicamos par√¢metro e peso.

> Para lembrar: Os pesos podem ser positivos ou negativos, dando mais ou menos for√ßa √†quela conex√£o.

A camada oculta vai tentar entender a informa√ß√£o recebida em diversos n√≠veis, dependendo do que foi configurado.

Cada um dos neur√¥nios da camada oculta tamb√©m se conectam aos da camada de sa√≠da. Em cada conex√£o, tambem temos um novo valor de pesos, que definir√° as ativa√ß√µes finais. Essas ativa√ß√µes finais representam o que de fato a rede identificou.

## Combina√ß√£o de Imagens para Formar um D√≠gito

<!-- 46:10 -->

Voltando ao exemplo do MNIST, vamos analisar as imagens abaixo:

![Aula01_Figura27](imagens/Aula01_Figura27.png)

Perceba que a imagem do n√∫mero `9` pode ser dividida em dois fragmentos, no primeiro temos um c√≠rculo, e no segundo um tra√ßo na vertical.

Um neur√¥nio √© respons√°vel por identificar o `primeiro fragmento` e outro neur√¥nio o `segundo fragmento`.

Juntos, ativam o neur√¥nio do `n√∫mero 9` na camada de sa√≠da. 

![Aula01_Figura28](imagens/Aula01_Figura28.png)

Para a imagem do n√∫mero `8` temos novamente dois fragmentos ‚Äî o mesmo c√≠rculo do `9` ‚Äî na por√ß√£o superior, e um outro c√≠rculo menor na por√ß√£o inferior.

> ***_NOTA :clipboard: :pencil2: :_***  As informa√ß√µes s√£o quebradas desta forma por uma quest√£o de utilidade.

![Aula01_Figura29](imagens/Aula01_Figura29.png)

Na imagem do n√∫mero `4`, o `primeiro fragmento` √© representado por uma linha vertical ‚Äî a mesmo vista no n√∫mero `9` ‚Äî, o `segundo fragmento` √© tamb√©m uma linha vertical e o `terceiro fragmento` representado por uma linha na horizontal.

O treinamento de reconhecimento destas imagens s√£o feitos ao mesmo tempo nas camadas ocultas de mais alto n√≠vel.

> **NOTA:** As camadas ocultas de mais alto n√≠vel s√£o aquelas que est√£o mais pr√≥ximas da camada de sa√≠da.

![Aula01_Figura30](imagens/Aula01_Figura30.png)

Assim, obteremos o seguinte resultado:

![Aula01_Figura31](imagens/Aula01_Figura31.png)

Cada um dos neur√¥nios guarda a informa√ß√£o da representa√ß√£o de cada um dos desenhos que vimos.

A ativa√ß√£o dos neur√¥nios em verde nos mostra que h√° grandes chances do n√∫mero `4` ser ativado. J√° ativa√ß√£o dos neur√¥nios em roxo, maiores chances de ser o n√∫mero `8`.

> :key: :bulb: At√© aqui entendemos que a rede neural coleta as informa√ß√µes que est√° observando, as combina, e toma alguma decis√£o a partir disso.

## Como cada peda√ßo de fragmento √© identificado?

<!-- 49:40 -->

Em m√©dia, um Ser Humano √© capaz de desenhar um c√≠rculo somente aos 3 anos de idade. Neste per√≠do, estamos aprendendo a coordenar o campo visual juntamente com a musculatura fina da m√£o-de-escrita para que o formato redondo saia no papel. √â uma tarefa complexa e desafiadora e o mesmo acontece no aprendizado da Rede Neural.

> **_Curiosidade: _** O c√≠rculo √© um s√≠mbolo universal com significado amplo, nos remete √†s no√ß√µes de totalidade, plenitude, perfei√ß√£o original, o Eu, o infinito, a eternidade. Interessante notar que uma das formas mais espalhadas na natureza exija tanto de n√≥s aprendermos seu formato. 

![Aula01_Figura32](imagens/Aula01_Figura32.png)

Note no exemplo abaixo que o entendimento do que √© um c√≠rculo pela Rede Neural seja necess√°ria a jun√ß√£o de 5 pedacinhos menores:

![Aula01_Figura33](imagens/Aula01_Figura33.png)

Lembra que anteriormente mencionamos que o aprendizado de algumas imagens √© feito em uma camada de alto n√≠vel?

![Aula01_Figura34](imagens/Aula01_Figura34.png)

Na primeira camada de neur√¥nios no exemplo acima, a informa√ß√£o que j√° foi aprendida √© fragmentada em pequenos peda√ßos e juntada com as informa√ß√µes da segunda camada para formar algo novo, como o formato do c√≠rculo que vimos no d√≠gito `9`. O mesmo acontece com a sua perninha, o seu reconhecimento tamb√©m √© fragmentado pela Rede Neural:

![Aula01_Figura35](imagens/Aula01_Figura35.png)

Para fins did√°ticos podemos dividir o processo de aprendizado do d√≠gito `9` em quatro etapas:

- Camada de Input: recebimento do d√≠gito `9` achatado;
- Primeira camada oculta: fragmentos do d√≠gito `9` s√£o reconhecidos e ativados;
- Segunda camada oculta: o d√≠gito `9` √© constru√≠do com apenas dois fragmentos ativando apenas dois neur√¥nios, e;
- Camada de sa√≠da: preenchimento do neur√¥nio correspondente ao d√≠gito `9`.
	
![Aula01_Figura36](imagens/Aula01_Figura36.png)

> :key: :bulb: Lembre-se que cada neur√¥nio √© um par√¢metro multiplicado por peso e bias.

## Evoluindo a fun√ß√£o do neur√¥nio

At√© aqui tratamos o neur√¥nio como **algo que guarda um n√∫mero**. Passando por todo o aprendizado vimos que seu trabalho vai al√©m disso: **√© uma fun√ß√£o matem√°tica**.

![Aula01_Figura37](imagens/Aula01_Figura37.png)

Por ser uma fun√ß√£o, o neur√¥nio agora vai **receber** n√∫meros e **transform√°-los** tornando todo o processo de aprendizagem mais flex√≠vel.

Vamos assimilar todas essas informa√ß√µes desta forma:

Recebemos o nosso neur√¥nio inicial com seus par√¢metros configurados. 

![Aula01_Figura38](imagens/Aula01_Figura38.png)

Para a nossa **primeira** camada oculta os par√¢metros s√£o a camada de entrada. 

O **primeiro** pixel ser√° multiplicado pelo peso aleat√≥rio `1`.

![Aula01_Figura39](imagens/Aula01_Figura39.png)

> :key: :bulb: Neste momento vamos assumir que o valor do peso √© definido de forma **aleat√≥ria**.

O **segundo** pixel ser√° multiplicado pelo peso aleat√≥rio `2`.

![Aula01_Figura40](imagens/Aula01_Figura40.png)

Vamos exemplificar com este c√°lculo:

- Par√¢metro 1 x Peso 1: `1 x 0 = 0`
- Par√¢metro 2 x Peso 2: `1 x 5 = 5`

Tendo em m√£os o resultado de cada par√¢metro, o resultado final ser√° como segue:

- `Total = 5`

![Aula01_Figura41](imagens/Aula01_Figura41.png)

> :key: :bulb: Para cada pixel entrando n√≥s repetimos sua multiplica√ß√£o pelo peso e o somamos aos que entraram na camada anteriormente

N√£o podemos esquecer de agregar o valor de bias que pode ser um valor negativo ou positivo, e que neste momento tamb√©m ser√° aleat√≥rio. 

- Par√¢metros + bias: `5 + 30 = 35`

![Aula01_Figura42](imagens/Aula01_Figura42.png)

> :key: :bulb: Os valores aleat√≥rios s√£o ajustados conforme a Rede Neural vai crescendo em aprendizado.Os valores aleat√≥rios s√£o ajustados conforme a Rede Neural vai crescendo em aprendizado.

Com o valor total da soma de par√¢metros + bias em m√£os, precisamos pass√°-lo por uma **fun√ß√£o de ativa√ß√£o** que o transformar√° em outro resultado. 

Neste momento esta fun√ß√£o dobra o valor recebido.

- Total anterior -> fun√ß√£o de ativa√ß√£o: `35 x 2 = 70`

![Aula01_Figura43](imagens/Aula01_Figura43.png)

> :key: :bulb: A forma de funcionamento da fun√ß√£o de ativa√ß√£o √© determinada por n√≥s. Por exemplo: por ela s√≥ seriam transformados n√∫meros pares.

O processo de **aprendizado da Rede Neural** pode ser entendido da atrav√©s de todas essas etapas que passamos. 

-> O neur√¥nio √© uma estrutura com uma fun√ß√£o de ativa√ß√£o.
-> Os pesos e bias inicialmente s√£o valores aleat√≥rios.
-> O valor de cada peso e bias √© ajustado conforme o aprendizado vai aumentando.

Em s√≠ntese, aprendizado √© ajustar os pesos e bias de forma correta e adequada. :key: :bulb:

![Aula01_Figura44](imagens/Aula01_Figura44.png)

## Determinando a quantidade de pesos em uma Rede Neural Profunda

<!-- 56:35 -->



## ‚ùóÔ∏è Links & Refer√™ncias usadas nesta aula

Esta aula no <a href="https://miro.com/app/board/o9J_ljr0G-g=/" target="_blank">Miro</a>

Site <a href="https://playground.tensorflow.org/" target="_blank">Tensorflow Playground</a>

Site <a href="https://www.3blue1brown.com/topics/neural-networks" target="_blank">3Blue1Brown</a>



## Pend√™ncias

<!-- CHANELY - CRIAR UM DICION√ÅRIO-R√ÅPIDO PARA OS TERMOS: (SALVATORE VALIDAR) -->

<!-- NEURONIO -->

<!-- CONEX√ïES -->

<!-- BIAS -->

<!-- FUN√á√ÉO DE ATIVA√á√ÉO -->

<!-- CAMADA DE ENTRADA -->

<!-- CAMADA OCULTA -->

<!-- CAMADA DE SA√çDA -->

<!-- FORMATO DE ENTRADA -->

<!-- PESOS -->

<!-- PROPAGA√á√ÉO -->

<!-- BACKPROPAGATION -->

<!-- TAXA DE APRENDIZAGEM -->

<!-- PRECIS√ÉO -->

<!-- ACUR√ÅCIA -->

<!-- SENSIBILIDADE -->

<!-- CONVERGENCIA -->

<!-- REGULARIZA√á√ÉO -->

<!-- NORMALIZA√á√ÉO -->

<!-- CAMADAS COMPLETAMENTE CONECTADAS -->

<!-- PERDA DE FUN√á√ÉO -->

<!-- OTIMIZA√á√ÉO DE MODELOS -->

<!-- METRICAS DE PERFORMANCE -->

<!-- BATCH SIZE -->

<!-- TRAINING EPOCHS -->
